stringsAsFactors = FALSE)
cluster_summary
is.na(NA)
is.na("")
(df)
count(df)
df <- df[rowSums(is.na(df)) <= n,]
df <- df[rowSums(is.na(df)) <= n]
df <- df[rowSums(is.na(df)) <= 0,]
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
df <- df[rowSums(is.na(df)) <= 0,]
count(df)
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
df <- df[rowSums(is.na(df)) <= 0,]
count(df)
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
df <- df[rowSums(is.na(df)) <= 0,]
count(df)
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 20)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 20, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
cluster_summary[[1]]
cluster_words[[1]]
cluster_words
summary(cluster_words)
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
df <- df[rowSums(is.na(df)) <= 0,]
count(df)
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 20)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 20, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
cluster_summary
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
df <- df[rowSums(is.na(df)) <= 0,]
count(df)
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
cluster_words[[1]]
summary(cluster_words[[1]])
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
df <- df[rowSums(is.na(df)) <= 0,]
count(df)
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
library(dplyr) #funny library name
library(textmineR)
df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
# create a document term matrix
dtm <- CreateDtm(doc_vec = df$title, # character vector of documents
doc_names = df$id, # document names
ngram_window = c(1, 2), # minimum and maximum n-gram length
stopword_vec = c(tm::stopwords("english"), # stopwords from tm
tm::stopwords("SMART")), # this is the default value
lower = TRUE, # lowercase - this is the default value
remove_punctuation = TRUE, # punctuation - this is the default
remove_numbers = TRUE, # numbers - this is the default
verbose = FALSE, # Turn off status bar for this demo
cpus = 2) # default is all available cpus on the system
# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf
tfidf <- t(tfidf)
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))
csim <- csim %*% t(csim)
cdist <- as.dist(1 - csim)
hc <- hclust(cdist, method="ward.D")
clustering <- cutree(hc, 10)
plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
ylab = "", xlab = "", yaxt = "n")
rect.hclust(hc, 10, border = "red")
p_words <- colSums(dtm) / sum(dtm)
cluster_words <- lapply(unique(clustering), function(x){
rows <- dtm[ clustering == x , ]
# for memory's sake, drop all words that don't appear in the cluster
rows <- rows[ , colSums(rows) > 0 ]
colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
wordcloud(words = names(cluster_words[[ 1 ]]),
freq = cluster_words[[ 1 ]],
max.words = 50,
random.order = FALSE,
colors = c("red", "yellow", "blue"),
main = "Top words in cluster 100")
names(cluster_words[[1]])
clustering
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
size = as.numeric(table(clustering)),
top_words = sapply(cluster_words, function(d){
paste(
names(d)[ order(d, decreasing = TRUE) ][ 1:5 ],
collapse = ", ")
}),
stringsAsFactors = FALSE)
cluster_summary
rawData$reportedAt
rawData <- read.csv("csv/cleaned_no_functionality.csv", na.strings=c(""))
rawData$reportedAt
typeof(rawData$reportedAt)
?rowMeans
rowMeans(rawData$ )
rowMeans(rawData$disclosed_at - rawData$created_at)
basic.data <- data.frame(rawData)
basic.data$timeToPatch <- basic.data$disclosed_at - basicData$reportedAt
basic.data <- data.frame(rawData)
basic.data$timeToPatch <- basic.data$disclosed_at - basicData$reportedAt
basic.data$timeToPatch <- basic.data$disclosed_at - basic.data$reportedAt
basic.data$timeToPatch <- (basic.data$disclosed_at - basic.data$reportedAt)
basic.data$timeToPatch
basic.data$timeToPatch <- (as.integer(basic.data$disclosed_at) - as.integer(basic.data$reportedAt))
basic.data$timeToPatch
count(is.na(basic.data$disclosed_at))
count(is.na(rawData$disclosed_at))
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$reportedAt
rawData$created_at
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$reportedAt
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$created_at
rawData$created_at<- strptime(rawData$createdAt, "%Y-%m-%d %H:%M:%S")
View(rawData)
rawData$created_at
rawData$created_at <- as.Date(rawData$created_at)
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$created_at <- as.Date(rawData$created_at)
rawData$reportedAt <- strptime(rawData$reportedAt, "%Y-%m-%d %H:%M:%S")
rawData$created_at<- strptime(rawData$createdAt, "%Y-%m-%d %H:%M:%S")
rawData$created_at
rawData$patched_at
rawData$patched_at
rawData$disclosed_at
rawData$disclosed_at
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$created_at <- as.Date(rawData$created_at)
rawData$disclosed_at <- as.Date(rawData$disclosed_at)
rawData$reportedAt <- strptime(rawData$reportedAt, "%Y-%m-%d %H:%M:%S")
rawData$disclosed_at
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$created_at <- as.Date(rawData$created_at)
rawData$disclosed_at <- as.Date(rawData$disclosed_at)
rawData$reportedAt <- strptime(rawData$reportedAt, "%Y-%m-%d %H:%M:%S")
rawData$disclosed_at <- strptime(rawData$disclosed_at, "%Y-%m-%d")
rawData$created_at <- strptime(rawData$created_at, "%Y-%m-%d")
rawData$disclosed_at
rawData$created_at
rawData$reportedAt
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$created_at <- as.Date(rawData$created_at)
rawData$disclosed_at <- as.Date(rawData$disclosed_at)
rawData$reportedAt <- strptime(rawData$reportedAt, "%Y-%m-%d %H:%M:%S")
rawData$disclosed_at <- strptime(rawData$disclosed_at, "%Y-%m-%d")
rawData$created_at <- strptime(rawData$created_at, "%Y-%m-%d")
basic.data <- data.frame(rawData)
count(is.na(rawData$disclosed_at))
basic.data <- data.frame(rawData)
basic.data$timeToPatch <- (as.integer(basic.data$disclosed_at) - as.integer(basic.data$reportedAt))
basic.data$timeToPatch <- difftime(basic.data$disclosed_at.time - basic.data$reportedAt.time)
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at.time - basic.data$reportedAt.time)
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at.time - basic.data$reportedAt.time)
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at.time - basic.data$reportedAt.time,units="secs")
basic.data$timeToPatch.time <- difftime(as.POSIXct(basic.data$disclosed_at) - as.POSIXct(basic.data$reportedAt),units="secs")
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(as.POSIXct(basic.data$disclosed_at) - as.POSIXct(basic.data$reportedAt),units="secs")
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData$created_at <- as.Date(rawData$created_at, format="%Y-%m-%d")
rawData$disclosed_at <- as.Date(rawData$disclosed_at, format="%Y-%m-%d")
rawData$reportedAt <- strptime(rawData$reportedAt, "%Y-%m-%d %H:%M:%S")
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(as.POSIXct(basic.data$disclosed_at.time) - as.POSIXct(basic.data$reportedAt),units="secs")
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(as.POSIXct(basic.data$disclosed_at) - as.POSIXct(basic.data$reportedAt),units="secs")
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(as.POSIXct(basic.data$disclosed_at) - as.POSIXct(basic.data$reportedAt),units="secs")
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at - basic.data$reportedAt,units="secs")
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at.time - basic.data$reportedAt.time,units="secs")
basic.data <- data.frame(rawData)
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at.time - basic.data$reportedAt.time,units="secs")
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at.time, basic.data$reportedAt.time,units="secs")
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at, basic.data$reportedAt.time,units="secs")
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at, basic.data$reportedAt,units="secs")
basic.data$timeToPatch
basic.data$timeToPatch.time <- difftime(basic.data$disclosed_at, basic.data$reportedAt,units="days")
basic.data$timeToPatch
count(is.na(basic.data.$disclosed_at))
count(is.na(basic.data$disclosed_at))
count(is.na(rawData$disclosed_at))
count(DF[rowSums(is.na(basic.data)) <= n,]  )
count(basic.data[rowSums(is.na(basic.data)) <= n,]  )
