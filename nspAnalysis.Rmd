---
title: "Insight: Node Security Issues"
author: "Ronauli Silva"
date: "06/29/2018"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## TLDR

This notebook is written in [R Markdown](http://rmarkdown.rstudio.com) Notebook. I guess the title is pretty much conveys the content :p

## How

So the idea is to see what's happening out there, see common issues, and it would be best to find it through a website that gather reports of node security issues. The best website I could find is [NodeSecurity.io](https://nodesecurity.io), and scraping the data. Thanks for Ramzi and Neel, by expanding the scraper range via connected link (github repo, hackerone report, npm keywords). 


### Data Source

1. Go to nodesecurity.io/advisories, we get:
    + Package name
    + Severity, level of damage. On range 1-10
    + Version, both vulnerable and patched one(if any)
    + Link to npm page of respective package
2. From NSP detail page, we get the hackerOne link.
3. From hackerone link, we gather these information as strings
    + Vulnerability, e.g: Interospection graphql query leaks sensitive data.
    + Impact, e.g: Execute bash script via request parameter
4. From npm page, we gather:
    + Keywords (easier to handle)
5. From Github API, we get:
    + Description/functionality of respective package
    
### How

So we run the scraper on node.js, and manage to find the insight using R.

## Insights

Initialization
```{r}
rawData <- read.csv("csv/cleaned_no_functionality.csv")
rawData
```

### Functionality with High Severity

```{r}
library(tm)
library(rvest)
library(SnowballC)
titleCorpus <- VCorpus(VectorSource(rawData$title))
clean <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
    return(corpus)
}

cleanFunctionalityData <- clean(titleCorpus)
# stemming

stemFunc <- content_transformer(function(x){
  paste(sapply(words(x),stemDocument),collapse = " ")
})

# stemmed functionality string
stemFnData <- tm_map(cleanFunctionalityData, stemFunc)
```

#### Bar chart: Most Frequent Word in Titles
```{r}
library(ggplot2)
dtm <- TermDocumentMatrix(stemFnData)
dtm
mat <- as.matrix(dtm)


# sort by frequency and print the first 20]
sortedData <- sort(rowSums(mat), decreasing = TRUE )
first20 <- sortedData[1:20];
d <- data.frame(word = names(sortedData), freq=sortedData)
# head(d, 20)

## Find a range of y's that'll leave sufficient space above the tallest bar
bpFreqWord <- barplot(sortedData[1:20]
        ,main='Most Frequent Word in Titles'
        ,horiz=TRUE
        ,xlab="Occurences"
        ,ylab="Word"
        ,col=rev(brewer.pal(9, "YlOrRd"))
        ,las=2
        ,cex.names=0.8)

ggplot(data =yy  , aes(x = word, y = freq, fill=-freq)) +
    geom_bar(stat="identity") +
    coord_flip() +
    scale_color_gradient2(low = "lightblue", high = "darkblue", midpoint=75) +
    labs(title = 'Word Occurences on Reports`s Title',
         x = 'Words',
         y = 'Occurences') 
```

#### Title - Severity Relationships

```{r}
library(dplyr) #funny library name
library(textmineR)

df <- data.frame(rawData$title, rawData$severity)
names(df) <- c("title","severity")
df <- mutate(df, id = rownames(df))
# create a document term matrix 
dtm <- CreateDtm(doc_vec = rawData2$title, # character vector of documents
                 doc_names = nih_sample$id, # document names
                 ngram_window = c(1, 2), # minimum and maximum n-gram length
                 stopword_vec = c(tm::stopwords("english"), # stopwords from tm
                                  tm::stopwords("SMART")), # this is the default value
                 lower = TRUE, # lowercase - this is the default value
                 remove_punctuation = TRUE, # punctuation - this is the default
                 remove_numbers = TRUE, # numbers - this is the default
                 verbose = FALSE, # Turn off status bar for this demo
                 cpus = 2) # default is all available cpus on the system

# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
```


#### Wordcloud
```{r}
a.dtm3 <- removeSparseTerms(dtm, sparse=0.7)
a.dtm.df <- as.data.frame(inspect(a.dtm3))
a.dtm.df.scale <- scale(a.dtm.df)
d <- dist(a.dtm.df.scale, method = "euclidean") 
fit <- hclust(d, method="ward")
plot(fit)


# just for fun... 
library(RColorBrewer)

m = as.matrix(t(dtm))
# get word counts in decreasing order
word_freqs = sort(colSums(m), decreasing=TRUE) 
# create a data frame with words and their frequencies
dm = data.frame(word=names(word_freqs), freq=word_freqs)
# plot wordcloud
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
```
Thank you.

